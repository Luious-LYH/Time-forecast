{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据，填充缺失值\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv('data/' + file_name, encoding='gbk')\n",
    "    columns = df.columns\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单变量单步 处理数据\n",
    "def nn_seq_us(B):\n",
    "    print('data processing...')\n",
    "    dataset = load_data(\"data.csv\")      # 读取的文件名\n",
    "    # split\n",
    "    train = dataset[:int(len(dataset) * 0.6)]\n",
    "    val = dataset[int(len(dataset) * 0.6):int(len(dataset) * 0.8)]\n",
    "    test = dataset[int(len(dataset) * 0.8):len(dataset)]\n",
    "    m, n = np.max(train[train.columns[1]]), np.min(train[train.columns[1]])\n",
    "\n",
    "    def process(data, batch_size, shuffle):\n",
    "        load = data[data.columns[1]]\n",
    "        load = load.tolist()\n",
    "        data = data.values.tolist()\n",
    "        load = (load - n) / (m - n)\n",
    "        seq = []\n",
    "        for i in range(len(data) - 24):\n",
    "            train_seq = []\n",
    "            train_label = []\n",
    "            for j in range(i, i + 24):\n",
    "                x = [load[j]]\n",
    "                train_seq.append(x)\n",
    "            # for c in range(2, 8):\n",
    "            #     train_seq.append(data[i + 24][c])\n",
    "            train_label.append(load[i + 24])\n",
    "            train_seq = torch.FloatTensor(train_seq)\n",
    "            train_label = torch.FloatTensor(train_label).view(-1)\n",
    "            seq.append((train_seq, train_label))\n",
    "\n",
    "        # print(seq[-1])\n",
    "        seq = MyDataset(seq)\n",
    "        seq = DataLoader(dataset=seq, batch_size=batch_size, shuffle=shuffle, num_workers=0, drop_last=True)\n",
    "\n",
    "        return seq\n",
    "\n",
    "    Dtr = process(train, B, True)\n",
    "    Val = process(val, B, True)\n",
    "    Dte = process(test, B, False)\n",
    "\n",
    "    return Dtr, Val, Dte, m, n\n",
    "#返回训练集、验证集、测试集、最大值和最小值。这些数据将用于神经网络的训练和评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算验证集的损失值\n",
    "def get_val_loss(args, model, Val):\n",
    "    model.eval()\n",
    "    loss_function = nn.MSELoss().to(args.device)\n",
    "    val_loss = []\n",
    "    for seq, label in Val:\n",
    "        with torch.no_grad():\n",
    "            seq = seq.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            y_pred = model(seq)\n",
    "            loss = loss_function(y_pred, label)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 用于设置随机种子，以确保在训练模型时的可重复性\n",
    "def setup_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(x, y):\n",
    "    \"\"\"\n",
    "    :param x: true value\n",
    "    :param y: pred value\n",
    "    :return: mape\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs((x - y) / x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "def train(args, Dtr, Val, path):\n",
    "    input_size, hidden_size, num_layers = args.input_size, args.hidden_size, args.num_layers\n",
    "    output_size = args.output_size\n",
    "    if args.bidirectional:\n",
    "        model = BiLSTM(input_size, hidden_size, num_layers, output_size, batch_size=args.batch_size).to(device)\n",
    "    else:\n",
    "        model = LSTM(input_size, hidden_size, num_layers, output_size, batch_size=args.batch_size).to(device)\n",
    "\n",
    "    loss_function = nn.MSELoss().to(device)\n",
    "    if args.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr,\n",
    "                                     weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\n",
    "                                    momentum=0.9, weight_decay=args.weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "    # training\n",
    "    min_epochs = 10\n",
    "    best_model = None\n",
    "    min_val_loss = 5\n",
    "    for epoch in tqdm(range(args.epochs)):\n",
    "        train_loss = []\n",
    "        for (seq, label) in Dtr:\n",
    "            seq = seq.to(device)\n",
    "            label = label.to(device)\n",
    "            y_pred = model(seq)\n",
    "            loss = loss_function(y_pred, label)\n",
    "            train_loss.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        # validation\n",
    "        val_loss = get_val_loss(args, model, Val)\n",
    "        if epoch > min_epochs and val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, np.mean(train_loss), val_loss))\n",
    "        model.train()\n",
    "\n",
    "    state = {'models': best_model.state_dict()}\n",
    "    torch.save(state, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "#测试模型\n",
    "def test(args, Dte, path, m, n):\n",
    "    pred = []\n",
    "    y = []\n",
    "    print('loading models...')\n",
    "    input_size, hidden_size, num_layers = args.input_size, args.hidden_size, args.num_layers\n",
    "    output_size = args.output_size\n",
    "    if args.bidirectional:\n",
    "        model = BiLSTM(input_size, hidden_size, num_layers, output_size, batch_size=args.batch_size).to(device)\n",
    "    else:\n",
    "        model = LSTM(input_size, hidden_size, num_layers, output_size, batch_size=args.batch_size).to(device)\n",
    "    # models = LSTM(input_size, hidden_size, num_layers, output_size, batch_size=args.batch_size).to(device)\n",
    "    model.load_state_dict(torch.load(path)['models'])\n",
    "    model.eval()\n",
    "    print('predicting...')\n",
    "    for (seq, target) in tqdm(Dte):\n",
    "        target = list(chain.from_iterable(target.data.tolist()))\n",
    "        y.extend(target)\n",
    "        seq = seq.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(seq)\n",
    "            y_pred = list(chain.from_iterable(y_pred.data.tolist()))\n",
    "            pred.extend(y_pred)\n",
    "\n",
    "    y, pred = np.array(y), np.array(pred)\n",
    "    y = (m - n) * y + n\n",
    "    pred = (m - n) * pred + n\n",
    "    print('mape:', get_mape(y, pred))\n",
    "    # plot\n",
    "    x = [i for i in range(1, 151)]\n",
    "    x_smooth = np.linspace(np.min(x), np.max(x), 900)\n",
    "    y_smooth = make_interp_spline(x, y[150:300])(x_smooth)\n",
    "    plt.plot(x_smooth, y_smooth, c='green', marker='*', ms=1, alpha=0.75, label='true')\n",
    "\n",
    "    y_smooth = make_interp_spline(x, pred[150:300])(x_smooth)\n",
    "    plt.plot(x_smooth, y_smooth, c='red', marker='o', ms=1, alpha=0.75, label='pred')\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.num_directions = 1 # 单向LSTM\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]\n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(device)\n",
    "        # output(batch_size, seq_len, num_directions * hidden_size)\n",
    "        output, _ = self.lstm(input_seq, (h_0, c_0)) # output(5, 30, 64)\n",
    "        pred = self.linear(output)  # (5, 30, 1)\n",
    "        pred = pred[:, -1, :]  # (5, 1)\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.num_directions = 2\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM((self.input_size), (self.hidden_size), (self.num_layers), batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_len = input_seq.shape[0], input_seq.shape[1]\n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        output, _ = self.lstm(input_seq, (h_0, c_0))\n",
    "        output = output.contiguous().view(batch_size, seq_len, self.num_directions, self.hidden_size)\n",
    "        output = torch.mean(output, dim=2)\n",
    "        pred = self.linear(output)\n",
    "        pred = pred[:, -1, :]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m curPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[0;32m      5\u001b[0m rootPath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(curPath)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(rootPath)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "curPath = os.path.abspath(os.path.dirname(__file__))\n",
    "rootPath = os.path.split(curPath)[0]\n",
    "sys.path.append(rootPath)\n",
    "\n",
    "from util import train, test, load_data\n",
    "from args import us_args_parser\n",
    "from data_process import setup_seed\n",
    "\n",
    "setup_seed(20)\n",
    "path = os.path.abspath(os.path.dirname(os.getcwd()))\n",
    "LSTM_PATH = path + '/models/univariate_single_step.pkl'\n",
    "# print(LSTM_PATH)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = us_args_parser()\n",
    "    #flag = 'us' # 表示使用单变量单步长的数据\n",
    "    Dtr, Val, Dte, m, n = load_data(args)  \n",
    "    train(args, Dtr, Val, LSTM_PATH)\n",
    "    test(args, Dte, LSTM_PATH, m, n)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Time_fore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
